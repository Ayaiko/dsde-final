{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d43dd4",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e391464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5cad62",
   "metadata": {},
   "source": [
    "## Step 1: Extract - Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40894bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.extract import load_traffy_data\n",
    "\n",
    "df_traffy = load_traffy_data('../data/raw/bangkok_traffy.csv')\n",
    "df_weather = pd.read_csv('../data/raw/open-meteo-13.74N100.50E9m.csv')\n",
    "\n",
    "print(f\"Traffy shape: {df_traffy.shape}\")\n",
    "print(f\"Weather shape: {df_weather.shape}\")\n",
    "\n",
    "df_traffy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538db20f",
   "metadata": {},
   "source": [
    "## Step 2: Transform - Clean Traffy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72086cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.utils import clean_traffy_data\n",
    "\n",
    "df_traffy = clean_traffy_data(df_traffy)\n",
    "print(f\"After cleaning: {df_traffy.shape}\")\n",
    "df_traffy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e7833",
   "metadata": {},
   "source": [
    "## Step 3: Transform - Split Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81583724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.utils import split_coordinates\n",
    "\n",
    "df_traffy = split_coordinates(df_traffy)\n",
    "print(f\"Coordinates split: {df_traffy.shape}\")\n",
    "print(f\"Lat range: {df_traffy['latitude'].min():.2f} to {df_traffy['latitude'].max():.2f}\")\n",
    "print(f\"Lon range: {df_traffy['longitude'].min():.2f} to {df_traffy['longitude'].max():.2f}\")\n",
    "df_traffy[['coords', 'latitude', 'longitude']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54add1",
   "metadata": {},
   "source": [
    "## Step 4: Transform - Parse Timestamps to Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traffy['timestamp'] = pd.to_datetime(df_traffy['timestamp'], format='mixed', utc=True)\n",
    "df_traffy['date'] = df_traffy['timestamp'].dt.date\n",
    "df_traffy['date'] = pd.to_datetime(df_traffy['date'])\n",
    "\n",
    "df_weather['date'] = pd.to_datetime(df_weather['time']).dt.date\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "\n",
    "print(f\"Traffy date range: {df_traffy['date'].min()} to {df_traffy['date'].max()}\")\n",
    "print(f\"Weather date range: {df_weather['date'].min()} to {df_weather['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd33956",
   "metadata": {},
   "source": [
    "## Step 5: Transform - Aggregate Weather to Daily Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_daily = df_weather.groupby('date').mean(numeric_only=True).reset_index()\n",
    "print(f\"Daily weather shape: {df_weather_daily.shape}\")\n",
    "df_weather_daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789ef74",
   "metadata": {},
   "source": [
    "## Step 6: Transform - Merge Traffy with Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_traffy.merge(df_weather_daily, on='date', how='left')\n",
    "\n",
    "temp_col = 'temperature_2m (°C)' if 'temperature_2m (°C)' in df_merged.columns else 'temperature_2m'\n",
    "match_rate = (~df_merged[temp_col].isna()).sum() / len(df_merged) * 100 if temp_col in df_merged.columns else 0\n",
    "\n",
    "print(f\"Merged shape: {df_merged.shape}\")\n",
    "print(f\"Weather match rate: {match_rate:.1f}%\")\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52dafc",
   "metadata": {},
   "source": [
    "## Step 7: Preprocess - Parse Type Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d76bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocess import parse_type_column\n",
    "\n",
    "df_merged = parse_type_column(df_merged)\n",
    "print(f\"Type column parsed: {df_merged.shape}\")\n",
    "print(f\"Sample types: {df_merged['type'].head().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c365bfd",
   "metadata": {},
   "source": [
    "## Step 8: Preprocess - Filter Empty Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f895010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocess import filter_empty_types\n",
    "\n",
    "df_merged = filter_empty_types(df_merged)\n",
    "print(f\"After filtering empty types: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530900d",
   "metadata": {},
   "source": [
    "## Step 9: Preprocess - Drop Missing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6542a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocess import drop_missing_weather\n",
    "\n",
    "df_merged = drop_missing_weather(df_merged)\n",
    "print(f\"After dropping missing weather: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035d3ff",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering - Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef8e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.features import prepare_features\n",
    "\n",
    "df_merged = prepare_features(df_merged)\n",
    "print(f\"After feature engineering: {df_merged.shape}\")\n",
    "print(f\"Feature columns: {[col for col in df_merged.columns if col.startswith(('hour_', 'day_', 'month_', 'district_'))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0af8c",
   "metadata": {},
   "source": [
    "## Step 11: Feature Engineering - Create Binary Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocess import create_binary_targets\n",
    "\n",
    "df_merged, binary_cols = create_binary_targets(df_merged)\n",
    "print(f\"Created {len(binary_cols)} binary target columns\")\n",
    "print(f\"Binary columns: {binary_cols[:10]}...\")\n",
    "print(f\"\\nFinal shape: {df_merged.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa404e",
   "metadata": {},
   "source": [
    "## Step 12: Optional - Sample Data for Faster Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.preprocess import sample_data\n",
    "\n",
    "df_sample = sample_data(df_merged, n=200000, random_state=42)\n",
    "print(f\"Sampled data: {df_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b981f",
   "metadata": {},
   "source": [
    "## Step 13: Explore Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = {col.replace('type_', ''): df_merged[col].sum() for col in binary_cols}\n",
    "type_counts = dict(sorted(type_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"Top 10 complaint types:\")\n",
    "for i, (type_name, count) in enumerate(list(type_counts.items())[:10], 1):\n",
    "    pct = count / len(df_merged) * 100\n",
    "    print(f\"{i:2d}. {type_name:20s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(range(len(type_counts)), list(type_counts.values()))\n",
    "plt.xticks(range(len(type_counts)), list(type_counts.keys()), rotation=45, ha='right')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Complaint Type Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483faa4",
   "metadata": {},
   "source": [
    "## Step 14: Train - Prepare Features for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99eb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.train import prepare_features_for_training\n",
    "\n",
    "X, feature_names = prepare_features_for_training(df_sample)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Feature names: {feature_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c807b9",
   "metadata": {},
   "source": [
    "## Step 15: Train - Get Trainable Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349e603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.train import get_trainable_types\n",
    "\n",
    "trainable_types = get_trainable_types(df_sample, min_samples=50)\n",
    "print(f\"Trainable types: {len(trainable_types)}\")\n",
    "print(f\"Types: {trainable_types[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa606b",
   "metadata": {},
   "source": [
    "## Step 16: Train - Train Single Type Model (Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.train import train_single_type_model\n",
    "\n",
    "demo_type = trainable_types[0]\n",
    "print(f\"Training model for: {demo_type}\")\n",
    "\n",
    "result = train_single_type_model(\n",
    "    df_sample,\n",
    "    type_col=f\"type_{demo_type}\",\n",
    "    feature_names=feature_names,\n",
    "    n_iter=3,\n",
    "    cv=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nModel trained successfully!\")\n",
    "print(f\"Accuracy: {result['metrics']['accuracy']:.4f}\")\n",
    "print(f\"Precision: {result['metrics']['precision']:.4f}\")\n",
    "print(f\"Recall: {result['metrics']['recall']:.4f}\")\n",
    "print(f\"F1 Score: {result['metrics']['f1']:.4f}\")\n",
    "print(f\"Best params: {result['metrics']['best_params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfa0b1",
   "metadata": {},
   "source": [
    "## Step 17: Train - Train All Types (Full Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.train import train_all_types\n",
    "\n",
    "results = train_all_types(\n",
    "    df_sample,\n",
    "    n_iter=5,\n",
    "    min_samples=50,\n",
    "    output_dir='../data/models',\n",
    "    adaptive_resampling=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete! Trained {len(results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e73cb",
   "metadata": {},
   "source": [
    "## Step 18: Results - Display Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "for type_name, result in results.items():\n",
    "    metrics = result['metrics']\n",
    "    summary_data.append({\n",
    "        'type': type_name,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1'],\n",
    "        'n_estimators': metrics['best_params'].get('n_estimators', None),\n",
    "        'max_depth': metrics['best_params'].get('max_depth', None)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('f1', ascending=False)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c637b",
   "metadata": {},
   "source": [
    "## Step 19: Visualize - Run Streamlit Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "print(\"Starting Streamlit dashboard...\")\n",
    "print(\"Dashboard will open in your browser at http://localhost:8501\")\n",
    "print(\"\\nPress Ctrl+C in the terminal to stop the server\\n\")\n",
    "\n",
    "subprocess.run(['streamlit', 'run', 'streamlit_app.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44421b9",
   "metadata": {},
   "source": [
    "## Step 20: Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ababdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "df_merged.to_csv('../data/processed/traffy_weather_daily.csv', index=False)\n",
    "print('Saved: data/processed/traffy_weather_daily.csv')\n",
    "print(f\"Final shape: {df_merged.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
