{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from pipeline.features import extract_time_features, encode_cyclical_features\n",
    "from pipeline.preprocess import parse_type_column, filter_empty_types, drop_missing_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dba389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: 651,600 records\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651600 entries, 0 to 651599\n",
      "Data columns (total 24 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   type                           651600 non-null  object \n",
      " 1   comment                        651600 non-null  object \n",
      " 2   coords                         651600 non-null  object \n",
      " 3   subdistrict                    651600 non-null  object \n",
      " 4   district                       651600 non-null  object \n",
      " 5   province                       651600 non-null  object \n",
      " 6   timestamp                      651600 non-null  object \n",
      " 7   longitude                      651600 non-null  float64\n",
      " 8   latitude                       651600 non-null  float64\n",
      " 9   timestamp_hour                 651600 non-null  object \n",
      " 10  grid_lat                       651600 non-null  float64\n",
      " 11  grid_lon                       651600 non-null  float64\n",
      " 12  time                           227639 non-null  object \n",
      " 13  temperature_2m (°C)            227639 non-null  float64\n",
      " 14  dew_point_2m (°C)              227639 non-null  float64\n",
      " 15  relative_humidity_2m (%)       227639 non-null  float64\n",
      " 16  rain (mm)                      227639 non-null  float64\n",
      " 17  vapour_pressure_deficit (kPa)  227639 non-null  float64\n",
      " 18  cloud_cover (%)                227639 non-null  float64\n",
      " 19  wind_direction_10m (°)         227639 non-null  float64\n",
      " 20  surface_pressure (hPa)         227639 non-null  float64\n",
      " 21  wind_speed_10m (km/h)          227639 non-null  float64\n",
      " 22  latitude_weather               227639 non-null  float64\n",
      " 23  longitude_weather              227639 non-null  float64\n",
      "dtypes: float64(15), object(9)\n",
      "memory usage: 119.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/traffy_weather_merged.csv')\n",
    "print(f\"Initial: {len(df):,} records\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4581c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter_empty_types: 540,633 records\n",
      "After drop_missing_weather: 187,138 records\n",
      "hello\n",
      "hello\n",
      "Final shape: (187138, 84), Types: 24\n",
      "Final shape: (187138, 84), Types: 24\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert weather columns to numeric\n",
    "weather_cols = ['pm25', 'pm10', 'o3', 'no2', 'temperature_2m (°C)', 'rain (mm)', 'wind_speed_10m (km/h)']\n",
    "for col in weather_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Parse and filter complaint types\n",
    "df = parse_type_column(df)\n",
    "df = filter_empty_types(df)\n",
    "print(f\"After filter_empty_types: {len(df):,} records\")\n",
    "\n",
    "# Drop rows with missing weather data\n",
    "df = drop_missing_weather(df, weather_columns=['temperature_2m (°C)', 'rain (mm)'])\n",
    "print(f\"After drop_missing_weather: {len(df):,} records\")\n",
    "\n",
    "# Extract temporal features\n",
    "df = extract_time_features(df, timestamp_col='timestamp')\n",
    "df = encode_cyclical_features(df)\n",
    "\n",
    "# One-hot encode districts\n",
    "district_encoded = pd.get_dummies(df['district'], prefix='district')\n",
    "df = pd.concat([df, district_encoded], axis=1)\n",
    "\n",
    "# Create binary target columns\n",
    "all_types = set()\n",
    "for type_list in df['type']:\n",
    "    all_types.update(type_list)\n",
    "\n",
    "for t in all_types:\n",
    "    df[f'type_{t}'] = df['type'].apply(lambda x: 1 if t in x else 0)\n",
    "\n",
    "print(f\"Final shape: {df.shape}, Types: {len(all_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f075ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total types: 24\n",
      "Top 10: ['ถนน', 'ทางเท้า', 'กีดขวาง', 'ความปลอดภัย', 'แสงสว่าง', 'ความสะอาด', 'จราจร', 'ท่อระบายน้ำ', 'สะพาน', 'ป้าย']\n"
     ]
    }
   ],
   "source": [
    "def get_predictable_types(df, sort_by_distribution=False):\n",
    "    \"\"\"\n",
    "    Get all predictable complaint types from dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with binary type columns (type_*)\n",
    "    sort_by_distribution : bool\n",
    "        If True, sort by frequency (most common first)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of all type column names (with 'type_' prefix)\n",
    "    \"\"\"\n",
    "    type_cols = [c for c in df.columns if c.startswith('type_')]\n",
    "    \n",
    "    if sort_by_distribution:\n",
    "        # Sort by count (most frequent first)\n",
    "        type_counts = [(col, df[col].sum()) for col in type_cols]\n",
    "        type_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        type_names = [col for col, count in type_counts]\n",
    "    else:\n",
    "        type_names = type_cols\n",
    "    \n",
    "    return type_names\n",
    "\n",
    "# Get list of all predictable types (sorted by frequency)\n",
    "available_types = get_predictable_types(df, sort_by_distribution=True)\n",
    "print(f\"Total types: {len(available_types)}\")\n",
    "print(f\"Top 10: {[t.replace('type_', '') for t in available_types[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c2b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_model(model, type_name, output_dir='data/models'):\n",
    "    \"\"\"\n",
    "    Save trained model to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model to save\n",
    "    type_name : str\n",
    "        Complaint type name (with 'type_' prefix)\n",
    "    output_dir : str\n",
    "        Directory to save models\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{type_name.replace('type_', '')}_model.pkl\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def train_models(df, type_indices, available_types, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train models for selected complaint types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Prepared dataframe with features and targets\n",
    "    type_indices : list\n",
    "        List of indices to select from available_types\n",
    "    available_types : list\n",
    "        List of all available type column names\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with type names as keys and results as values\n",
    "    \"\"\"\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Parameter distribution for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 501),\n",
    "        'max_depth': randint(10, 51),\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample']\n",
    "    }\n",
    "    \n",
    "    # Prepare features once\n",
    "    exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "    X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "    \n",
    "    for idx in type_indices:\n",
    "        if idx >= len(available_types):\n",
    "            print(f\"⚠ Index {idx} out of range, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        target = available_types[idx]\n",
    "        type_name = target.replace('type_', '')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Training model for: {type_name} (index {idx})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        y = df[target]\n",
    "        minority_ratio = y.sum() / len(y)\n",
    "        print(f\"Positive samples: {y.sum():,} ({minority_ratio:.2%})\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if minority_ratio < 0.05:\n",
    "            try:\n",
    "                smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "                under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "                X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "                print(f\"After resampling: {len(X_res):,} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Resampling failed: {e}, using original data\")\n",
    "                X_res, y_res = X, y\n",
    "        else:\n",
    "            X_res, y_res = X, y\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train with RandomizedSearchCV\n",
    "        print(f\"Training with RandomizedSearchCV (n_iter={n_iter})...\")\n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        random_search = RandomizedSearchCV(rf, param_dist, n_iter=n_iter, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=0)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        model = random_search.best_estimator_\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'best_params': random_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        filepath = save_model(model, target)\n",
    "        \n",
    "        print(f\"\\n✓ Results:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "        print(f\"  Saved to:  {filepath}\")\n",
    "        \n",
    "        results[type_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'filepath': filepath\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42983804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected types for training:\n",
      "  [0] คลอง\n",
      "\n",
      "================================================================================\n",
      "Training model for: คลอง (index 0)\n",
      "================================================================================\n",
      "Positive samples: 4,207 (2.25%)\n",
      "After resampling: 158,539 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n",
      "After resampling: 158,539 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n",
      "\n",
      "✓ Results:\n",
      "  Accuracy:  0.9716\n",
      "  Precision: 0.9836\n",
      "  Recall:    0.8929\n",
      "  F1:        0.9360\n",
      "  Saved to:  data/models\\คลอง_model.pkl\n",
      "\n",
      "================================================================================\n",
      "Training Summary\n",
      "================================================================================\n",
      "คลอง: F1=0.9360, saved to data/models\\คลอง_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Select types to train (by index)\n",
    "selected_indices = [0]  # Train top 3 types\n",
    "\n",
    "print(f\"Selected types for training:\")\n",
    "for idx in selected_indices:\n",
    "    if idx < len(available_types):\n",
    "        print(f\"  [{idx}] {available_types[idx].replace('type_', '')}\")\n",
    "\n",
    "# Train models for selected types\n",
    "trained_results = train_models(df, selected_indices, available_types, n_iter=1)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\"*80)\n",
    "for type_name, result in trained_results.items():\n",
    "    print(f\"{type_name}: F1={result['metrics']['f1']:.4f}, saved to {result['filepath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d82ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather features (4): ['temperature_2m (°C)', 'rain (mm)', 'wind_direction_10m (°)', 'wind_speed_10m (km/h)']\n",
      "\n",
      "Target: type_ถนน (44,676, 23.87%)\n",
      "Train: 149,710, Test: 37,428, Features: 50\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "target = 'type_ถนน'\n",
    "exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "\n",
    "X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "y = df[target]\n",
    "\n",
    "# Show weather features being used\n",
    "weather_cols = [c for c in X.columns if any(w in c.lower() for w in ['pm25', 'pm10', 'temperature', 'rain', 'wind', 'o3', 'no2'])]\n",
    "print(f\"Weather features ({len(weather_cols)}): {weather_cols[:10]}\")\n",
    "\n",
    "minority_ratio = y.sum() / len(y)\n",
    "print(f\"\\nTarget: {target} ({y.sum():,}, {minority_ratio:.2%})\")\n",
    "\n",
    "if minority_ratio < 0.05:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "    under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "    X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "else:\n",
    "    X_res, y_res = X, y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}, Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72df7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "Best params: {'class_weight': 'balanced', 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 120}\n",
      "Accuracy: 0.6127\n",
      "Precision: 0.3174\n",
      "Recall: 0.5212\n",
      "F1: 0.3945\n",
      "\n",
      "Best params: {'class_weight': 'balanced', 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 120}\n",
      "Accuracy: 0.6127\n",
      "Precision: 0.3174\n",
      "Recall: 0.5212\n",
      "F1: 0.3945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 501),\n",
    "    'max_depth': randint(10, 51),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'class_weight': ['balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(rf, param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "model = random_search.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"\\nBest params: {random_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
