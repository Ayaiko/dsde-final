{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd8b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Pipeline functions\n",
    "def extract_time_features(df, timestamp_col='timestamp'):\n",
    "    df = df.copy()\n",
    "    df['timestamp_col'] = pd.to_datetime(df[timestamp_col], format='mixed', utc=True)\n",
    "    df['hour'] = df['timestamp_col'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp_col'].dt.dayofweek\n",
    "    df['month'] = df['timestamp_col'].dt.month\n",
    "    return df\n",
    "\n",
    "def encode_cyclical_features(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "def parse_type_column(df):\n",
    "    def parse_cell(cell):\n",
    "        if isinstance(cell, str):\n",
    "            s = cell.strip('{}').strip()\n",
    "            return [x.strip() for x in s.split(',')] if s else []\n",
    "        elif isinstance(cell, list):\n",
    "            return cell\n",
    "        else:\n",
    "            return []\n",
    "    df['type'] = df['type'].apply(parse_cell)\n",
    "    return df\n",
    "\n",
    "def filter_empty_types(df, column='type'):\n",
    "    return df[df[column].map(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "\n",
    "def drop_missing_weather(df, weather_columns=None):\n",
    "    if weather_columns is None:\n",
    "        weather_columns = ['temperature_2m (°C)', 'rain (mm)', 'wind_speed_10m (km/h)']\n",
    "    return df.dropna(subset=weather_columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76dba389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grain\\AppData\\Local\\Temp\\ipykernel_9704\\507818388.py:1: DtypeWarning: Columns (8,9,10,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/processed/traffy_weather_merged.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: 651,600 records\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651600 entries, 0 to 651599\n",
      "Data columns (total 22 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   type                           651600 non-null  object \n",
      " 1   comment                        651600 non-null  object \n",
      " 2   coords                         651600 non-null  object \n",
      " 3   subdistrict                    651600 non-null  object \n",
      " 4   district                       651600 non-null  object \n",
      " 5   province                       651600 non-null  object \n",
      " 6   timestamp                      651600 non-null  object \n",
      " 7   date                           651600 non-null  object \n",
      " 8    pm25                          651138 non-null  object \n",
      " 9    pm10                          651138 non-null  object \n",
      " 10   o3                            651138 non-null  object \n",
      " 11   no2                           651138 non-null  object \n",
      " 12  type_list                      651600 non-null  object \n",
      " 13  temperature_2m (°C)            651600 non-null  float64\n",
      " 14  dew_point_2m (°C)              651600 non-null  float64\n",
      " 15  relative_humidity_2m (%)       651600 non-null  float64\n",
      " 16  rain (mm)                      651600 non-null  float64\n",
      " 17  vapour_pressure_deficit (kPa)  651600 non-null  float64\n",
      " 18  cloud_cover (%)                651600 non-null  float64\n",
      " 19  wind_direction_10m (°)         651600 non-null  float64\n",
      " 20  surface_pressure (hPa)         651600 non-null  float64\n",
      " 21  wind_speed_10m (km/h)          651600 non-null  float64\n",
      "dtypes: float64(9), object(13)\n",
      "memory usage: 109.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/traffy_weather_merged.csv')\n",
    "print(f\"Initial: {len(df):,} records\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d0f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 200k records for faster training\n",
    "\n",
    "def sample(df):\n",
    "    df_sampled = df.sample(n=200000, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Sampled to: {len(df_sampled):,} records\")\n",
    "    return df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e4581c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled to: 200,000 records\n",
      "After filter_empty_types: 200,000 records\n",
      "After drop_missing_weather: 200,000 records\n",
      "After filter_empty_types: 200,000 records\n",
      "After drop_missing_weather: 200,000 records\n",
      "Found 25 unique types\n",
      "Found 25 unique types\n",
      "Created 26 binary type columns\n",
      "Sample type column dtypes: [dtype('O'), dtype('int64'), dtype('int64')]\n",
      "Final shape: (200000, 107), Types: 25\n",
      "Created 26 binary type columns\n",
      "Sample type column dtypes: [dtype('O'), dtype('int64'), dtype('int64')]\n",
      "Final shape: (200000, 107), Types: 25\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "df = sample(df)\n",
    "\n",
    "# Convert weather columns to numeric\n",
    "weather_cols = ['pm25', 'pm10', 'o3', 'no2', 'temperature_2m (°C)', 'rain (mm)', 'wind_speed_10m (km/h)']\n",
    "for col in weather_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Parse and filter complaint types\n",
    "df = parse_type_column(df)\n",
    "df = filter_empty_types(df)\n",
    "print(f\"After filter_empty_types: {len(df):,} records\")\n",
    "\n",
    "# Drop rows with missing weather data\n",
    "df = drop_missing_weather(df, weather_columns=['temperature_2m (°C)', 'rain (mm)'])\n",
    "print(f\"After drop_missing_weather: {len(df):,} records\")\n",
    "\n",
    "# Extract temporal features\n",
    "df = extract_time_features(df, timestamp_col='timestamp')\n",
    "df = encode_cyclical_features(df)\n",
    "\n",
    "# One-hot encode districts\n",
    "district_encoded = pd.get_dummies(df['district'], prefix='district')\n",
    "df = pd.concat([df, district_encoded], axis=1)\n",
    "\n",
    "# Create binary target columns\n",
    "all_types = set()\n",
    "for type_list in df['type']:\n",
    "    if isinstance(type_list, list):\n",
    "        all_types.update(type_list)\n",
    "    else:\n",
    "        print(f\"Warning: Non-list type found: {type_list}\")\n",
    "\n",
    "print(f\"Found {len(all_types)} unique types\")\n",
    "\n",
    "for t in all_types:\n",
    "    df[f'type_{t}'] = df['type'].apply(lambda x: 1 if (isinstance(x, list) and t in x) else 0)\n",
    "\n",
    "# Verify binary columns are numeric\n",
    "type_cols = [c for c in df.columns if c.startswith('type_')]\n",
    "print(f\"Created {len(type_cols)} binary type columns\")\n",
    "print(f\"Sample type column dtypes: {df[type_cols[:3]].dtypes.tolist()}\")\n",
    "\n",
    "print(f\"Final shape: {df.shape}, Types: {len(all_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f075ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total types: 26\n",
      "Top 10: ['ถนน', 'ไม่ระบุ', 'ทางเท้า', 'ความปลอดภัย', 'แสงสว่าง', 'กีดขวาง', 'ความสะอาด', 'จราจร', 'น้ำท่วม', 'ท่อระบายน้ำ']\n"
     ]
    }
   ],
   "source": [
    "def get_predictable_types(df, sort_by_distribution=False):\n",
    "    \"\"\"\n",
    "    Get all predictable complaint types from dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with binary type columns (type_*)\n",
    "    sort_by_distribution : bool\n",
    "        If True, sort by frequency (most common first)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of all type column names (with 'type_' prefix)\n",
    "    \"\"\"\n",
    "    type_cols = [c for c in df.columns if c.startswith('type_')]\n",
    "    \n",
    "    if sort_by_distribution:\n",
    "        # Count occurrences for each type and sort\n",
    "        type_counts = {}\n",
    "        for col in type_cols:\n",
    "            type_counts[col] = (df[col] == 1).sum()\n",
    "        \n",
    "        # Sort by count (most frequent first)\n",
    "        sorted_types = sorted(type_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        type_names = [col for col, count in sorted_types]\n",
    "    else:\n",
    "        type_names = type_cols\n",
    "    \n",
    "    return type_names\n",
    "\n",
    "# Get list of all predictable types (sorted by frequency)\n",
    "available_types = get_predictable_types(df, sort_by_distribution=True)\n",
    "print(f\"Total types: {len(available_types)}\")\n",
    "print(f\"Top 10: {[t.replace('type_', '') for t in available_types[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c2b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_model(model, type_name, output_dir='data/models'):\n",
    "    \"\"\"\n",
    "    Save trained model to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model to save\n",
    "    type_name : str\n",
    "        Complaint type name (with 'type_' prefix)\n",
    "    output_dir : str\n",
    "        Directory to save models\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{type_name.replace('type_', '')}_model.pkl\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def save_feature_names(feature_names, output_dir='data/models'):\n",
    "    \"\"\"Save feature names to a separate file (same for all models).\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, 'feature_names.pkl')\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    return filepath\n",
    "\n",
    "def train_models(df, type_indices, available_types, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train models for selected complaint types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Prepared dataframe with features and targets\n",
    "    type_indices : list\n",
    "        List of indices to select from available_types\n",
    "    available_types : list\n",
    "        List of all available type column names\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with type names as keys and results as values\n",
    "    \"\"\"\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Parameter distribution for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 501),\n",
    "        'max_depth': randint(10, 51),\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample']\n",
    "    }\n",
    "    \n",
    "    # Prepare features once\n",
    "    exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "    X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Save feature names once (same for all models)\n",
    "    feature_names_path = save_feature_names(feature_names)\n",
    "    print(f\"✓ Feature names saved to: {feature_names_path}\")\n",
    "    print(f\"  Total features: {len(feature_names)}\\n\")\n",
    "    \n",
    "    for idx in type_indices:\n",
    "        if idx >= len(available_types):\n",
    "            print(f\"⚠ Index {idx} out of range, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        target = available_types[idx]\n",
    "        type_name = target.replace('type_', '')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Training model for: {type_name} (index {idx})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        y = df[target]\n",
    "        minority_ratio = y.sum() / len(y)\n",
    "        print(f\"Positive samples: {y.sum():,} ({minority_ratio:.2%})\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if minority_ratio < 0.05:\n",
    "            try:\n",
    "                smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "                under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "                X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "                print(f\"After resampling: {len(X_res):,} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Resampling failed: {e}, using original data\")\n",
    "                X_res, y_res = X, y\n",
    "        else:\n",
    "            X_res, y_res = X, y\n",
    "        \n",
    "        # Train/test split with stratification\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "        )\n",
    "          \n",
    "        # Train with RandomizedSearchCV\n",
    "        print(f\"Training with RandomizedSearchCV (n_iter={n_iter})...\")\n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        random_search = RandomizedSearchCV(rf, param_dist, n_iter=n_iter, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=0)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        model = random_search.best_estimator_\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'best_params': random_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        filepath = save_model(model, target)\n",
    "        \n",
    "        print(f\"\\n✓ Results:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "        print(f\"  Saved to:  {filepath}\")\n",
    "        \n",
    "        # Feature importance with names\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Feature Importances:\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {row['feature']:40s} {row['importance']:.6f}\")\n",
    "        \n",
    "        results[type_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'filepath': filepath,\n",
    "            'feature_importance': importance_df\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42983804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected types for training:\n",
      "  [22] การเดินทาง\n",
      "✓ Feature names saved to: data/models\\feature_names.pkl\n",
      "  Total features: 72\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training model for: การเดินทาง (index 22)\n",
      "================================================================================\n",
      "Positive samples: 569 (0.28%)\n",
      "After resampling: 172,839 samples\n",
      "Training with RandomizedSearchCV (n_iter=10)...\n",
      "After resampling: 172,839 samples\n",
      "Training with RandomizedSearchCV (n_iter=10)...\n",
      "\n",
      "✓ Results:\n",
      "  Accuracy:  0.9963\n",
      "  Precision: 0.9968\n",
      "  Recall:    0.9870\n",
      "  F1:        0.9919\n",
      "  Saved to:  data/models\\การเดินทาง_model.pkl\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "  day_cos                                  0.119967\n",
      "  day_sin                                  0.111447\n",
      "  hour_cos                                 0.081166\n",
      "  hour_sin                                 0.077425\n",
      "  day_of_week                              0.057089\n",
      "  month_cos                                0.038451\n",
      "  month_sin                                0.035362\n",
      "  hour                                     0.029170\n",
      "  cloud_cover (%)                          0.028321\n",
      "  wind_direction_10m (°)                   0.025107\n",
      "\n",
      "================================================================================\n",
      "Training Summary\n",
      "================================================================================\n",
      "การเดินทาง: F1=0.9919, saved to data/models\\การเดินทาง_model.pkl\n",
      "\n",
      "✓ Results:\n",
      "  Accuracy:  0.9963\n",
      "  Precision: 0.9968\n",
      "  Recall:    0.9870\n",
      "  F1:        0.9919\n",
      "  Saved to:  data/models\\การเดินทาง_model.pkl\n",
      "\n",
      "Top 10 Feature Importances:\n",
      "  day_cos                                  0.119967\n",
      "  day_sin                                  0.111447\n",
      "  hour_cos                                 0.081166\n",
      "  hour_sin                                 0.077425\n",
      "  day_of_week                              0.057089\n",
      "  month_cos                                0.038451\n",
      "  month_sin                                0.035362\n",
      "  hour                                     0.029170\n",
      "  cloud_cover (%)                          0.028321\n",
      "  wind_direction_10m (°)                   0.025107\n",
      "\n",
      "================================================================================\n",
      "Training Summary\n",
      "================================================================================\n",
      "การเดินทาง: F1=0.9919, saved to data/models\\การเดินทาง_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Select types to train (by index)\n",
    "selected_indices = [22]  # Train top 3 types\n",
    "\n",
    "print(f\"Selected types for training:\")\n",
    "for idx in selected_indices:\n",
    "    if idx < len(available_types):\n",
    "        print(f\"  [{idx}] {available_types[idx].replace('type_', '')}\")\n",
    "\n",
    "# Train models for selected types\n",
    "trained_results = train_models(df, selected_indices, available_types, n_iter=10)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\"*80)\n",
    "for type_name, result in trained_results.items():\n",
    "    print(f\"{type_name}: F1={result['metrics']['f1']:.4f}, saved to {result['filepath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d82ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# target = 'type_ถนน'\n",
    "# exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "\n",
    "# X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "# y = df[target]\n",
    "\n",
    "# # Show weather features being used\n",
    "# weather_cols = [c for c in X.columns if any(w in c.lower() for w in ['pm25', 'pm10', 'temperature', 'rain', 'wind', 'o3', 'no2'])]\n",
    "# print(f\"Weather features ({len(weather_cols)}): {weather_cols[:10]}\")\n",
    "\n",
    "# minority_ratio = y.sum() / len(y)\n",
    "# print(f\"\\nTarget: {target} ({y.sum():,}, {minority_ratio:.2%})\")\n",
    "\n",
    "# if minority_ratio < 0.05:\n",
    "#     smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "#     under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "#     X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "# else:\n",
    "#     X_res, y_res = X, y\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "# print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}, Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72df7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 501),\n",
    "#     'max_depth': randint(10, 51),\n",
    "#     'min_samples_split': randint(2, 11),\n",
    "#     'min_samples_leaf': randint(1, 5),\n",
    "#     'max_features': ['sqrt', 'log2', None],\n",
    "#     'class_weight': ['balanced', 'balanced_subsample']\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# random_search = RandomizedSearchCV(rf, param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=1)\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# model = random_search.best_estimator_\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(f\"\\nBest params: {random_search.best_params_}\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"F1: {f1_score(y_test, y_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
