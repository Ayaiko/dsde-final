{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd8b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Pipeline functions\n",
    "def extract_time_features(df, timestamp_col='timestamp'):\n",
    "    df = df.copy()\n",
    "    df['timestamp_col'] = pd.to_datetime(df[timestamp_col], format='mixed', utc=True)\n",
    "    df['hour'] = df['timestamp_col'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp_col'].dt.dayofweek\n",
    "    df['month'] = df['timestamp_col'].dt.month\n",
    "    return df\n",
    "\n",
    "def encode_cyclical_features(df):\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "def parse_type_column(df):\n",
    "    def parse_cell(cell):\n",
    "        if isinstance(cell, str):\n",
    "            s = cell.strip('{}').strip()\n",
    "            return [x.strip() for x in s.split(',')] if s else []\n",
    "        elif isinstance(cell, list):\n",
    "            return cell\n",
    "        else:\n",
    "            return []\n",
    "    df['type'] = df['type'].apply(parse_cell)\n",
    "    return df\n",
    "\n",
    "def filter_empty_types(df, column='type'):\n",
    "    return df[df[column].map(lambda x: len(x) > 0)].reset_index(drop=True)\n",
    "\n",
    "def drop_missing_weather(df, weather_columns=None):\n",
    "    if weather_columns is None:\n",
    "        weather_columns = ['temperature_2m (°C)', 'rain (mm)', 'wind_speed_10m (km/h)']\n",
    "    return df.dropna(subset=weather_columns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76dba389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: 651,600 records\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651600 entries, 0 to 651599\n",
      "Data columns (total 24 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   type                           651600 non-null  object \n",
      " 1   comment                        651600 non-null  object \n",
      " 2   coords                         651600 non-null  object \n",
      " 3   subdistrict                    651600 non-null  object \n",
      " 4   district                       651600 non-null  object \n",
      " 5   province                       651600 non-null  object \n",
      " 6   timestamp                      651600 non-null  object \n",
      " 7   longitude                      651600 non-null  float64\n",
      " 8   latitude                       651600 non-null  float64\n",
      " 9   timestamp_hour                 651600 non-null  object \n",
      " 10  grid_lat                       651600 non-null  float64\n",
      " 11  grid_lon                       651600 non-null  float64\n",
      " 12  time                           227639 non-null  object \n",
      " 13  temperature_2m (°C)            227639 non-null  float64\n",
      " 14  dew_point_2m (°C)              227639 non-null  float64\n",
      " 15  relative_humidity_2m (%)       227639 non-null  float64\n",
      " 16  rain (mm)                      227639 non-null  float64\n",
      " 17  vapour_pressure_deficit (kPa)  227639 non-null  float64\n",
      " 18  cloud_cover (%)                227639 non-null  float64\n",
      " 19  wind_direction_10m (°)         227639 non-null  float64\n",
      " 20  surface_pressure (hPa)         227639 non-null  float64\n",
      " 21  wind_speed_10m (km/h)          227639 non-null  float64\n",
      " 22  latitude_weather               227639 non-null  float64\n",
      " 23  longitude_weather              227639 non-null  float64\n",
      "dtypes: float64(15), object(9)\n",
      "memory usage: 119.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651600 entries, 0 to 651599\n",
      "Data columns (total 24 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   type                           651600 non-null  object \n",
      " 1   comment                        651600 non-null  object \n",
      " 2   coords                         651600 non-null  object \n",
      " 3   subdistrict                    651600 non-null  object \n",
      " 4   district                       651600 non-null  object \n",
      " 5   province                       651600 non-null  object \n",
      " 6   timestamp                      651600 non-null  object \n",
      " 7   longitude                      651600 non-null  float64\n",
      " 8   latitude                       651600 non-null  float64\n",
      " 9   timestamp_hour                 651600 non-null  object \n",
      " 10  grid_lat                       651600 non-null  float64\n",
      " 11  grid_lon                       651600 non-null  float64\n",
      " 12  time                           227639 non-null  object \n",
      " 13  temperature_2m (°C)            227639 non-null  float64\n",
      " 14  dew_point_2m (°C)              227639 non-null  float64\n",
      " 15  relative_humidity_2m (%)       227639 non-null  float64\n",
      " 16  rain (mm)                      227639 non-null  float64\n",
      " 17  vapour_pressure_deficit (kPa)  227639 non-null  float64\n",
      " 18  cloud_cover (%)                227639 non-null  float64\n",
      " 19  wind_direction_10m (°)         227639 non-null  float64\n",
      " 20  surface_pressure (hPa)         227639 non-null  float64\n",
      " 21  wind_speed_10m (km/h)          227639 non-null  float64\n",
      " 22  latitude_weather               227639 non-null  float64\n",
      " 23  longitude_weather              227639 non-null  float64\n",
      "dtypes: float64(15), object(9)\n",
      "memory usage: 119.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/processed/traffy_weather_merged.csv')\n",
    "print(f\"Initial: {len(df):,} records\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e4581c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter_empty_types: 540,633 records\n",
      "After drop_missing_weather: 187,138 records\n",
      "Final shape: (187138, 84), Types: 24\n",
      "Final shape: (187138, 84), Types: 24\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert weather columns to numeric\n",
    "weather_cols = ['pm25', 'pm10', 'o3', 'no2', 'temperature_2m (°C)', 'rain (mm)', 'wind_speed_10m (km/h)']\n",
    "for col in weather_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Parse and filter complaint types\n",
    "df = parse_type_column(df)\n",
    "df = filter_empty_types(df)\n",
    "print(f\"After filter_empty_types: {len(df):,} records\")\n",
    "\n",
    "# Drop rows with missing weather data\n",
    "df = drop_missing_weather(df, weather_columns=['temperature_2m (°C)', 'rain (mm)'])\n",
    "print(f\"After drop_missing_weather: {len(df):,} records\")\n",
    "\n",
    "# Extract temporal features\n",
    "df = extract_time_features(df, timestamp_col='timestamp')\n",
    "df = encode_cyclical_features(df)\n",
    "\n",
    "# One-hot encode districts\n",
    "district_encoded = pd.get_dummies(df['district'], prefix='district')\n",
    "df = pd.concat([df, district_encoded], axis=1)\n",
    "\n",
    "# Create binary target columns\n",
    "all_types = set()\n",
    "for type_list in df['type']:\n",
    "    all_types.update(type_list)\n",
    "\n",
    "for t in all_types:\n",
    "    df[f'type_{t}'] = df['type'].apply(lambda x: 1 if t in x else 0)\n",
    "\n",
    "print(f\"Final shape: {df.shape}, Types: {len(all_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f075ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total types: 24\n",
      "Top 10: ['ถนน', 'ทางเท้า', 'กีดขวาง', 'ความปลอดภัย', 'แสงสว่าง', 'ความสะอาด', 'จราจร', 'ท่อระบายน้ำ', 'สะพาน', 'ป้าย']\n"
     ]
    }
   ],
   "source": [
    "def get_predictable_types(df, sort_by_distribution=False):\n",
    "    \"\"\"\n",
    "    Get all predictable complaint types from dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with binary type columns (type_*)\n",
    "    sort_by_distribution : bool\n",
    "        If True, sort by frequency (most common first)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of all type column names (with 'type_' prefix)\n",
    "    \"\"\"\n",
    "    type_cols = [c for c in df.columns if c.startswith('type_')]\n",
    "    \n",
    "    if sort_by_distribution:\n",
    "        # Sort by count (most frequent first)\n",
    "        type_counts = [(col, df[col].sum()) for col in type_cols]\n",
    "        type_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        type_names = [col for col, count in type_counts]\n",
    "    else:\n",
    "        type_names = type_cols\n",
    "    \n",
    "    return type_names\n",
    "\n",
    "# Get list of all predictable types (sorted by frequency)\n",
    "available_types = get_predictable_types(df, sort_by_distribution=True)\n",
    "print(f\"Total types: {len(available_types)}\")\n",
    "print(f\"Top 10: {[t.replace('type_', '') for t in available_types[:10]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33c2b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_model(model, type_name, output_dir='data/models'):\n",
    "    \"\"\"\n",
    "    Save trained model to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : sklearn model\n",
    "        Trained model to save\n",
    "    type_name : str\n",
    "        Complaint type name (with 'type_' prefix)\n",
    "    output_dir : str\n",
    "        Directory to save models\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{type_name.replace('type_', '')}_model.pkl\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def save_feature_names(feature_names, output_dir='data/models'):\n",
    "    \"\"\"Save feature names to a separate file (same for all models).\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filepath = os.path.join(output_dir, 'feature_names.pkl')\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(feature_names, f)\n",
    "    return filepath\n",
    "\n",
    "def train_models(df, type_indices, available_types, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train models for selected complaint types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Prepared dataframe with features and targets\n",
    "    type_indices : list\n",
    "        List of indices to select from available_types\n",
    "    available_types : list\n",
    "        List of all available type column names\n",
    "    n_iter : int\n",
    "        Number of RandomizedSearchCV iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with type names as keys and results as values\n",
    "    \"\"\"\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Parameter distribution for RandomizedSearchCV\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(100, 501),\n",
    "        'max_depth': randint(10, 51),\n",
    "        'min_samples_split': randint(2, 11),\n",
    "        'min_samples_leaf': randint(1, 5),\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample']\n",
    "    }\n",
    "    \n",
    "    # Prepare features once\n",
    "    exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "    X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Save feature names once (same for all models)\n",
    "    feature_names_path = save_feature_names(feature_names)\n",
    "    print(f\"✓ Feature names saved to: {feature_names_path}\")\n",
    "    print(f\"  Total features: {len(feature_names)}\\n\")\n",
    "    \n",
    "    for idx in type_indices:\n",
    "        if idx >= len(available_types):\n",
    "            print(f\"⚠ Index {idx} out of range, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        target = available_types[idx]\n",
    "        type_name = target.replace('type_', '')\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Training model for: {type_name} (index {idx})\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        y = df[target]\n",
    "        minority_ratio = y.sum() / len(y)\n",
    "        print(f\"Positive samples: {y.sum():,} ({minority_ratio:.2%})\")\n",
    "        \n",
    "        # Resample if needed\n",
    "        if minority_ratio < 0.05:\n",
    "            try:\n",
    "                smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "                under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "                X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "                print(f\"After resampling: {len(X_res):,} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Resampling failed: {e}, using original data\")\n",
    "                X_res, y_res = X, y\n",
    "        else:\n",
    "            X_res, y_res = X, y\n",
    "        \n",
    "        # Train/test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train with RandomizedSearchCV\n",
    "        print(f\"Training with RandomizedSearchCV (n_iter={n_iter})...\")\n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "        random_search = RandomizedSearchCV(rf, param_dist, n_iter=n_iter, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=0)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        model = random_search.best_estimator_\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'best_params': random_search.best_params_\n",
    "        }\n",
    "        \n",
    "        # Save model\n",
    "        filepath = save_model(model, target)\n",
    "        \n",
    "        print(f\"\\n✓ Results:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1:        {metrics['f1']:.4f}\")\n",
    "        print(f\"  Saved to:  {filepath}\")\n",
    "        \n",
    "        # Feature importance with names\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Feature Importances:\")\n",
    "        for i, row in importance_df.head(10).iterrows():\n",
    "            print(f\"  {row['feature']:40s} {row['importance']:.6f}\")\n",
    "        \n",
    "        results[type_name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'filepath': filepath,\n",
    "            'feature_importance': importance_df\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42983804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected types for training:\n",
      "  [22] การเดินทาง\n",
      "✓ Feature names saved to: data/models\\feature_names.pkl\n",
      "  Total features: 50\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training model for: การเดินทาง (index 22)\n",
      "================================================================================\n",
      "Positive samples: 650 (0.35%)\n",
      "After resampling: 161,620 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n",
      "After resampling: 161,620 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected types for training:\n",
      "  [22] การเดินทาง\n",
      "✓ Feature names saved to: data/models\\feature_names.pkl\n",
      "  Total features: 50\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training model for: การเดินทาง (index 22)\n",
      "================================================================================\n",
      "Positive samples: 650 (0.35%)\n",
      "After resampling: 161,620 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n",
      "After resampling: 161,620 samples\n",
      "Training with RandomizedSearchCV (n_iter=1)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_types[idx].replace(\u001b[33m'\u001b[39m\u001b[33mtype_\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train models for selected types\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m trained_results = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Summary\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(df, type_indices, available_types, n_iter)\u001b[39m\n\u001b[32m    115\u001b[39m rf = RandomForestClassifier(random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m    116\u001b[39m random_search = RandomizedSearchCV(rf, param_dist, n_iter=n_iter, cv=\u001b[32m3\u001b[39m, scoring=\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[43mrandom_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    120\u001b[39m model = random_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1992\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1991\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1992\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\grain\\anaconda3\\envs\\dsde-cedt\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Select types to train (by index)\n",
    "selected_indices = [22]  # Train top 3 types\n",
    "\n",
    "print(f\"Selected types for training:\")\n",
    "for idx in selected_indices:\n",
    "    if idx < len(available_types):\n",
    "        print(f\"  [{idx}] {available_types[idx].replace('type_', '')}\")\n",
    "\n",
    "# Train models for selected types\n",
    "trained_results = train_models(df, selected_indices, available_types, n_iter=1)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Summary\")\n",
    "print(\"=\"*80)\n",
    "for type_name, result in trained_results.items():\n",
    "    print(f\"{type_name}: F1={result['metrics']['f1']:.4f}, saved to {result['filepath']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d82ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# target = 'type_ถนน'\n",
    "# exclude = df.select_dtypes(include=['object']).columns.tolist() + ['timestamp', 'timestamp_col'] + [c for c in df.columns if c.startswith('type_')]\n",
    "\n",
    "# X = df[[c for c in df.columns if c not in exclude]].fillna(0)\n",
    "# y = df[target]\n",
    "\n",
    "# # Show weather features being used\n",
    "# weather_cols = [c for c in X.columns if any(w in c.lower() for w in ['pm25', 'pm10', 'temperature', 'rain', 'wind', 'o3', 'no2'])]\n",
    "# print(f\"Weather features ({len(weather_cols)}): {weather_cols[:10]}\")\n",
    "\n",
    "# minority_ratio = y.sum() / len(y)\n",
    "# print(f\"\\nTarget: {target} ({y.sum():,}, {minority_ratio:.2%})\")\n",
    "\n",
    "# if minority_ratio < 0.05:\n",
    "#     smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy=0.2)\n",
    "#     under = RandomUnderSampler(random_state=42, sampling_strategy=0.3)\n",
    "#     X_res, y_res = ImbPipeline([('s', smote), ('u', under)]).fit_resample(X, y)\n",
    "# else:\n",
    "#     X_res, y_res = X, y\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "# print(f\"Train: {len(X_train):,}, Test: {len(X_test):,}, Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df7726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 501),\n",
    "#     'max_depth': randint(10, 51),\n",
    "#     'min_samples_split': randint(2, 11),\n",
    "#     'min_samples_leaf': randint(1, 5),\n",
    "#     'max_features': ['sqrt', 'log2', None],\n",
    "#     'class_weight': ['balanced', 'balanced_subsample']\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "# random_search = RandomizedSearchCV(rf, param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1, verbose=1)\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# model = random_search.best_estimator_\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(f\"\\nBest params: {random_search.best_params_}\")\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "# print(f\"F1: {f1_score(y_test, y_pred):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsde-cedt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
